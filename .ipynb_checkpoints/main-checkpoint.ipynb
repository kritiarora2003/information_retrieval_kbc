{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COLIEE 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation of Task 1 of the 2024 Competition on Legal Information and Extraction/Entailment (COLIEE) by Damian Curran and Mike Conway.\n",
    "\n",
    "Details of the implementation can be found in our paper 'Similarity Ranking of Case Law Using Propositions as Features' (2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from helper python files.\n",
    "\n",
    "import t5train_code, file_code, pairs_code, model_code\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t5 Proposition Extraction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune t5-base on training data\n",
    "\n",
    "importlib.reload(t5train_code)\n",
    "from t5train_code import get_trainer, train_save_model\n",
    "\n",
    "trainer = get_trainer()\n",
    "train_save_model(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(file_code)\n",
    "from file_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw data from text files. Generating files dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read-in train files: 100%|█████████████████| 7350/7350 [00:10<00:00, 687.59it/s]\n",
      "Read-in test files: 100%|██████████████████| 2159/2159 [00:05<00:00, 410.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning \"files\" df.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = get_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>set</th>\n",
       "      <th>query</th>\n",
       "      <th>cases</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>059961</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;FRAGMENT_SUPPRESSED&gt;   &lt;FRAGMENT_SUPPRESSED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>058794</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n  &lt;FRAGMENT_SUPPRESSED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>057680</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]\\nLeBlanc, J.\\n: This is a judicial review ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>075584</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>[067672.txt, 019050.txt, 095238.txt, 047989.txt]</td>\n",
       "      <td>I.\\nIntroduction\\n[1]\\nShore, J.\\n[Translation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>085456</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;FRAGMENT_SUPPRESSED&gt;   &lt;FRAGMENT_SUPPRESSED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9284</th>\n",
       "      <td>076100</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]\\nGagné, J.\\n: This application for judicia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9285</th>\n",
       "      <td>035730</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;FRAGMENT_SUPPRESSED&gt; \\nFederal Court\\nMactav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9286</th>\n",
       "      <td>005193</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1]\\nMartineau, J.\\n: This is an application f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9287</th>\n",
       "      <td>053188</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;FRAGMENT_SUPPRESSED&gt; \\nFederal Court\\nO'Keef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9288</th>\n",
       "      <td>072311</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Reasons For Judgment\\n[1]\\nDubé, J.\\n[Original...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9289 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename    set  query                                             cases  \\\n",
       "0      059961  train  False                                                []   \n",
       "1      058794  train  False                                                []   \n",
       "2      057680  train  False                                                []   \n",
       "3      075584  train   True  [067672.txt, 019050.txt, 095238.txt, 047989.txt]   \n",
       "4      085456  train  False                                                []   \n",
       "...       ...    ...    ...                                               ...   \n",
       "9284   076100   test  False                                                []   \n",
       "9285   035730   test  False                                                []   \n",
       "9286   005193   test  False                                                []   \n",
       "9287   053188   test   True                                                []   \n",
       "9288   072311   test  False                                                []   \n",
       "\n",
       "                                                   text  \n",
       "0       <FRAGMENT_SUPPRESSED>   <FRAGMENT_SUPPRESSED...  \n",
       "1      \\n \\n \\n \\n \\n \\n \\n \\n  <FRAGMENT_SUPPRESSED...  \n",
       "2     [1]\\nLeBlanc, J.\\n: This is a judicial review ...  \n",
       "3     I.\\nIntroduction\\n[1]\\nShore, J.\\n[Translation...  \n",
       "4       <FRAGMENT_SUPPRESSED>   <FRAGMENT_SUPPRESSED...  \n",
       "...                                                 ...  \n",
       "9284  [1]\\nGagné, J.\\n: This application for judicia...  \n",
       "9285   <FRAGMENT_SUPPRESSED> \\nFederal Court\\nMactav...  \n",
       "9286  [1]\\nMartineau, J.\\n: This is an application f...  \n",
       "9287   <FRAGMENT_SUPPRESSED> \\nFederal Court\\nO'Keef...  \n",
       "9288  Reasons For Judgment\\n[1]\\nDubé, J.\\n[Original...  \n",
       "\n",
       "[9289 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>set</th>\n",
       "      <th>query</th>\n",
       "      <th>cases</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>032547</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>I.\\nBackground\\n[1]\\nHugessen, J.\\n: This is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>085437</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>-\\nPara.\\nINTRODUCTION\\n1\\n--I. NATURE OF THE ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename    set  query cases  \\\n",
       "7174   032547   test   True    []   \n",
       "13     085437  train  False    []   \n",
       "\n",
       "                                                   text  \n",
       "7174  I.\\nBackground\\n[1]\\nHugessen, J.\\n: This is a...  \n",
       "13    -\\nPara.\\nINTRODUCTION\\n1\\n--I. NATURE OF THE ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row_true = files_all[files_all['query'] == True].sample(n=1)\n",
    "# row_false = files_all[files_all['query'] == False].sample(n=1)\n",
    "\n",
    "# # Combine them\n",
    "# files = pd.concat([row_true, row_false])\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text from files. Extracting paragraphs based on regex pattern.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting all paragraphs: 100%|██████████████████| 2/2 [00:00<00:00, 43.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated \"files\" df has with \"paragraphs\".\n",
      "Getting formatted paragraphs of length < 250 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting formatted paragraphs: 100%|███████████████| 2/2 [00:00<00:00, 24.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added formatted paragraphs to \"files\" df in \"paragraphs_formatted\".\n",
      "Using regex and spacy (for long paragraphs) to extract and modify suppressed sections from paragraphs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting suppressed sections from paragraphs: 100%|█| 2/2 [00:00<00:00, 687.14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added suppressed sections to \"suppressed_sections\" field.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_paragraphs(files)\n",
    "get_paragraphs_formatted(files)\n",
    "add_suppressed_sections(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>set</th>\n",
       "      <th>query</th>\n",
       "      <th>cases</th>\n",
       "      <th>text</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>paragraphs_formatted</th>\n",
       "      <th>suppressed_sections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>032547</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>I.\\nBackground\\n[1]\\nHugessen, J.\\n: This is a...</td>\n",
       "      <td>[[1] Hugessen, J. : This is an action in which...</td>\n",
       "      <td>[Hugessen, J. : This is an action in which the...</td>\n",
       "      <td>[[13] The Supreme Court of Canada has confirme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>085437</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>-\\nPara.\\nINTRODUCTION\\n1\\n--I. NATURE OF THE ...</td>\n",
       "      <td>[[1] de Montigny, J. [Translation]: On Decembe...</td>\n",
       "      <td>[de Montigny, J. [Translation]: On December 23...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename    set  query cases  \\\n",
       "7174   032547   test   True    []   \n",
       "13     085437  train  False    []   \n",
       "\n",
       "                                                   text  \\\n",
       "7174  I.\\nBackground\\n[1]\\nHugessen, J.\\n: This is a...   \n",
       "13    -\\nPara.\\nINTRODUCTION\\n1\\n--I. NATURE OF THE ...   \n",
       "\n",
       "                                             paragraphs  \\\n",
       "7174  [[1] Hugessen, J. : This is an action in which...   \n",
       "13    [[1] de Montigny, J. [Translation]: On Decembe...   \n",
       "\n",
       "                                   paragraphs_formatted  \\\n",
       "7174  [Hugessen, J. : This is an action in which the...   \n",
       "13    [de Montigny, J. [Translation]: On December 23...   \n",
       "\n",
       "                                    suppressed_sections  \n",
       "7174  [[13] The Supreme Court of Canada has confirme...  \n",
       "13                                                   []  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting suppressed sections: 100%|████████████| 2/2 [00:00<00:00, 535.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_suppressed_formatted(files):\n",
    "    # logging.info('Getting formatted paragraphs of length < 250 words from suppressed_sections.')\n",
    "\n",
    "    def extract_paragraphs_formatted(row):\n",
    "        # Only keep paragraphs with 2+ words and < 250 words\n",
    "        paragraphs_formatted = [clean_text(p) for p in row['suppressed_sections'] if len(p.split()) < 250]\n",
    "        paragraphs_formatted = [p for p in paragraphs_formatted if len(p.split()) > 1]\n",
    "        return paragraphs_formatted\n",
    "\n",
    "    tqdm.pandas(desc=\"Formatting suppressed sections\")\n",
    "    files['propositions'] = files.progress_apply(extract_paragraphs_formatted, axis=1)\n",
    "\n",
    "    # logging.info('Added formatted paragraphs to \"suppressed_formatted\" column.')\n",
    "get_suppressed_formatted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>set</th>\n",
       "      <th>query</th>\n",
       "      <th>cases</th>\n",
       "      <th>text</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>paragraphs_formatted</th>\n",
       "      <th>suppressed_sections</th>\n",
       "      <th>propositions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>032547</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>I.\\nBackground\\n[1]\\nHugessen, J.\\n: This is a...</td>\n",
       "      <td>[[1] Hugessen, J. : This is an action in which...</td>\n",
       "      <td>[Hugessen, J. : This is an action in which the...</td>\n",
       "      <td>[[13] The Supreme Court of Canada has confirme...</td>\n",
       "      <td>[The Supreme Court of Canada has confirmed tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>085437</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>-\\nPara.\\nINTRODUCTION\\n1\\n--I. NATURE OF THE ...</td>\n",
       "      <td>[[1] de Montigny, J. [Translation]: On Decembe...</td>\n",
       "      <td>[de Montigny, J. [Translation]: On December 23...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename    set  query cases  \\\n",
       "7174   032547   test   True    []   \n",
       "13     085437  train  False    []   \n",
       "\n",
       "                                                   text  \\\n",
       "7174  I.\\nBackground\\n[1]\\nHugessen, J.\\n: This is a...   \n",
       "13    -\\nPara.\\nINTRODUCTION\\n1\\n--I. NATURE OF THE ...   \n",
       "\n",
       "                                             paragraphs  \\\n",
       "7174  [[1] Hugessen, J. : This is an action in which...   \n",
       "13    [[1] de Montigny, J. [Translation]: On Decembe...   \n",
       "\n",
       "                                   paragraphs_formatted  \\\n",
       "7174  [Hugessen, J. : This is an action in which the...   \n",
       "13    [de Montigny, J. [Translation]: On December 23...   \n",
       "\n",
       "                                    suppressed_sections  \\\n",
       "7174  [[13] The Supreme Court of Canada has confirme...   \n",
       "13                                                   []   \n",
       "\n",
       "                                           propositions  \n",
       "7174  [The Supreme Court of Canada has confirmed tha...  \n",
       "13                                                   []  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using language detection model to filter non-English propositions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjain/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Getting English propositions: 100%|███████████████| 2/2 [00:12<00:00,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added English-only propositions to \"files\" df in \"propositions_en\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_english_propositions(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using spacy to extract sentences of char length > 25 from paragraphs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting sentences from paragraphs: 100%|███████| 2/2 [00:13<00:00,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added lists of sentences to \"sentences\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_sentences(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>set</th>\n",
       "      <th>query</th>\n",
       "      <th>cases</th>\n",
       "      <th>text</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>paragraphs_formatted</th>\n",
       "      <th>suppressed_sections</th>\n",
       "      <th>propositions</th>\n",
       "      <th>propositions_en</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>032547</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>I.\\nBackground\\n[1]\\nHugessen, J.\\n: This is a...</td>\n",
       "      <td>[[1] Hugessen, J. : This is an action in which...</td>\n",
       "      <td>[Hugessen, J. : This is an action in which the...</td>\n",
       "      <td>[[13] The Supreme Court of Canada has confirme...</td>\n",
       "      <td>[The Supreme Court of Canada has confirmed tha...</td>\n",
       "      <td>[The Supreme Court of Canada has confirmed tha...</td>\n",
       "      <td>[This is an action in which the plaintiffs see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>085437</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>-\\nPara.\\nINTRODUCTION\\n1\\n--I. NATURE OF THE ...</td>\n",
       "      <td>[[1] de Montigny, J. [Translation]: On Decembe...</td>\n",
       "      <td>[de Montigny, J. [Translation]: On December 23...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[de Montigny, J. [Translation]: On December 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename    set  query cases  \\\n",
       "7174   032547   test   True    []   \n",
       "13     085437  train  False    []   \n",
       "\n",
       "                                                   text  \\\n",
       "7174  I.\\nBackground\\n[1]\\nHugessen, J.\\n: This is a...   \n",
       "13    -\\nPara.\\nINTRODUCTION\\n1\\n--I. NATURE OF THE ...   \n",
       "\n",
       "                                             paragraphs  \\\n",
       "7174  [[1] Hugessen, J. : This is an action in which...   \n",
       "13    [[1] de Montigny, J. [Translation]: On Decembe...   \n",
       "\n",
       "                                   paragraphs_formatted  \\\n",
       "7174  [Hugessen, J. : This is an action in which the...   \n",
       "13    [de Montigny, J. [Translation]: On December 23...   \n",
       "\n",
       "                                    suppressed_sections  \\\n",
       "7174  [[13] The Supreme Court of Canada has confirme...   \n",
       "13                                                   []   \n",
       "\n",
       "                                           propositions  \\\n",
       "7174  [The Supreme Court of Canada has confirmed tha...   \n",
       "13                                                   []   \n",
       "\n",
       "                                        propositions_en  \\\n",
       "7174  [The Supreme Court of Canada has confirmed tha...   \n",
       "13                                                   []   \n",
       "\n",
       "                                              sentences  \n",
       "7174  [This is an action in which the plaintiffs see...  \n",
       "13    [de Montigny, J. [Translation]: On December 23...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "1717\n"
     ]
    }
   ],
   "source": [
    "for s in files['sentences'].iloc[:2]:\n",
    "    print(len(s))   # how many sentences per row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.to_csv('just_before_english_sentences_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjain/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 85884490-31c9-4f23-94f4-848c23711b78)')' thrown while requesting HEAD https://huggingface.co/papluca/xlm-roberta-base-language-detection/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 85884490-31c9-4f23-94f4-848c23711b78)')' thrown while requesting HEAD https://huggingface.co/papluca/xlm-roberta-base-language-detection/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Device\n",
    "# device = \"cpu\"\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"papluca/xlm-roberta-base-language-detection\"\n",
    "# ).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Batch size for processing sentences (adjust if memory is low)\n",
    "# BATCH_SIZE = 128\n",
    "\n",
    "# # Helper: check which sentences are English\n",
    "# def custom_get_english_sections(sections, tokenizer, model, device, batch_size=BATCH_SIZE):\n",
    "#     english_sentences = []\n",
    "\n",
    "#     # Split sentences into batches\n",
    "#     for i in range(0, len(sections), batch_size):\n",
    "#         batch = sections[i:i+batch_size]\n",
    "#         # Tokenize batch\n",
    "#         inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#             # Get predicted language id\n",
    "#             preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "#         # The model's config id2label maps language IDs to ISO codes\n",
    "#         for sentence, pred_id in zip(batch, preds):\n",
    "#             lang = model.config.id2label[pred_id]\n",
    "#             if lang.lower() == \"en\":  # Keep English sentences\n",
    "#                 english_sentences.append(sentence)\n",
    "\n",
    "#     return english_sentences\n",
    "\n",
    "# # Apply to your DataFrame\n",
    "# def custom_get_english_sections(sections, tokenizer, model, device, batch_size=BATCH_SIZE):\n",
    "#     english_sentences = []\n",
    "\n",
    "#     for i in range(0, len(sections), batch_size):\n",
    "#         batch = sections[i:i+batch_size]\n",
    "#         print(f\"Processing batch {i//batch_size + 1} of {len(sections)//batch_size + 1}...\")  # DEBUG\n",
    "\n",
    "#         inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#             preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "#         for sentence, pred_id in zip(batch, preds):\n",
    "#             lang = model.config.id2label[pred_id]\n",
    "#             if lang.lower() == \"en\":\n",
    "#                 english_sentences.append(sentence)\n",
    "\n",
    "#     return english_sentences\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"papluca/xlm-roberta-base-language-detection\"\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Batch size for processing sentences (adjust if memory is low)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# # Helper: check which sentences are English\n",
    "# def custom_get_english_sections(sections, tokenizer, model, device, batch_size=BATCH_SIZE):\n",
    "#     english_sentences = []\n",
    "\n",
    "#     # Split sentences into batches\n",
    "#     for i in range(0, len(sections), batch_size):\n",
    "#         batch = sections[i:i+batch_size]\n",
    "#         # Tokenize batch\n",
    "#         inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#             # Get predicted language id\n",
    "#             preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "#         # The model's config id2label maps language IDs to ISO codes\n",
    "#         for sentence, pred_id in zip(batch, preds):\n",
    "#             lang = model.config.id2label[pred_id]\n",
    "#             if lang.lower() == \"en\":  # Keep English sentences\n",
    "#                 english_sentences.append(sentence)\n",
    "\n",
    "#     return english_sentences\n",
    "\n",
    "# # Apply to your DataFrame\n",
    "# def custom_get_english_sentences(files):\n",
    "#     import logging\n",
    "#     logging.info('Using language detection model to filter non-English sentences.')\n",
    "\n",
    "#     tqdm.pandas(desc=\"Getting English sentences\")\n",
    "#     files['sentences_en'] = files['sentences'].progress_apply(\n",
    "#         lambda sections: custom_get_english_sections(sections, tokenizer, model, device)\n",
    "#     )\n",
    "\n",
    "#     logging.info('Added English-only sentences to \"sentences_en\" column.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_get_english_sections(sections, tokenizer, model, device, batch_size=BATCH_SIZE):\n",
    "    english_sentences = []\n",
    "\n",
    "    for i in range(0, len(sections), batch_size):\n",
    "        batch = sections[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1} of {len(sections)//batch_size + 1}...\")  # DEBUG\n",
    "\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "        for sentence, pred_id in zip(batch, preds):\n",
    "            lang = model.config.id2label[pred_id]\n",
    "            if lang.lower() == \"en\":\n",
    "                english_sentences.append(sentence)\n",
    "\n",
    "    return english_sentences\n",
    "\n",
    "# Apply to your DataFrame\n",
    "def custom_get_english_sentences(files):\n",
    "    import logging\n",
    "    logging.info('Using language detection model to filter non-English sentences.')\n",
    "\n",
    "    tqdm.pandas(desc=\"Getting English sentences\")\n",
    "    files['sentences_en'] = files['sentences'].progress_apply(\n",
    "        lambda sections: custom_get_english_sections(sections, tokenizer, model, device)\n",
    "    )\n",
    "\n",
    "    logging.info('Added English-only sentences to \"sentences_en\" column.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using language detection model to filter non-English sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting English sentences:   0%|                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 2...\n",
      "Processing batch 2 of 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting English sentences: 100%|██████████████████| 2/2 [00:58<00:00, 29.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 14...\n",
      "Processing batch 2 of 14...\n",
      "Processing batch 3 of 14...\n",
      "Processing batch 4 of 14...\n",
      "Processing batch 5 of 14...\n",
      "Processing batch 6 of 14...\n",
      "Processing batch 7 of 14...\n",
      "Processing batch 8 of 14...\n",
      "Processing batch 9 of 14...\n",
      "Processing batch 10 of 14...\n",
      "Processing batch 11 of 14...\n",
      "Processing batch 12 of 14...\n",
      "Processing batch 13 of 14...\n",
      "Processing batch 14 of 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting English sentences: 100%|█████████████████| 2/2 [11:56<00:00, 358.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added English-only sentences to \"sentences_en\" column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "custom_get_english_sentences(files)\n",
    "files.to_csv('get_english_sentences_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using regex to extract quotations from suppressed sections:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting quotes from suppressed sections:: 100%|█| 2/2 [00:00<00:00, 237.45it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added quotes to the \"quotes\" field for query cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_quotes(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using spacy to extract noun entities, from English sentences:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities from english sentences: 100%|█| 2/2 [00:13<00:00,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added entity strings to \"entity_string\" and entities as sets to \"entity_set\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_entities(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting case word strings (for tfidf) and sets (for case jaccard):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting case string and sets from sentences_en:: 100%|█| 2/2 [00:00<00:00,  3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added case strings and sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_strings_sets(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting set lists:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting set list sentences_en:: 100%|██████████| 2/2 [00:00<00:00,  3.31it/s]\n",
      "Extracting set list paragraphs_formatted:: 100%|██| 2/2 [00:00<00:00,  3.87it/s]\n",
      "Extracting set list from propositions_en:: 100%|██| 2/2 [00:00<00:00, 69.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added set lists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_set_lists(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using regex to extract judge surname from first paragraphs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting judge surname from paragraphs: 100%|█| 2/2 [00:00<00:00, 1877.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added judge name to \"judge\" field.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_judge_name(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using string search to find year:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting most recent year from file text: 100%|█| 2/2 [00:00<00:00, 83.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added year to files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_year(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m files\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings from sentences, paragraphs and propositions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjain/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Getting embeddings for sentences_en: 100%|███████| 2/2 [06:53<00:00, 206.56s/it]\n",
      "Getting embeddings for paragraphs formatted: 100%|█| 2/2 [03:56<00:00, 118.30s/i\n",
      "Getting embeddings for propositions_en: 100%|█████| 2/2 [00:07<00:00,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_embeddings(files)\n",
    "files.to_csv('embeddings_all.csv', index=False)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pairs_code)\n",
    "from pairs_code import (get_pairs, add_bins, get_prop_max_cos_sim_sents, get_prop_max_cos_sim_paras,\n",
    "                        get_prop_max_jaccard_sents, get_prop_max_jaccard_paras, get_prop_max_overlap_sents, get_prop_max_overlap_paras, add_max_overall,\n",
    "                        get_case_jaccard_sims, check_same_case, get_case_tfidf_scores, get_num_quotes, binarize_quotes, check_years, add_judge_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pairs dataframe from files.\n"
     ]
    }
   ],
   "source": [
    "# Generate pairs dataframe. One query-candidate case pair per row. Compare file features from files df to generate pair features:\n",
    "\n",
    "pairs = get_pairs(files)\n",
    "get_prop_max_cos_sim_sents(files, pairs)\n",
    "get_prop_max_cos_sim_paras(files, pairs)\n",
    "get_prop_max_jaccard_sents(files,pairs)\n",
    "get_prop_max_jaccard_paras(files,pairs)\n",
    "get_prop_max_overlap_sents(files,pairs)\n",
    "get_prop_max_overlap_paras(files,pairs)\n",
    "add_max_overall(pairs,files)\n",
    "get_case_jaccard_sims(files,pairs)\n",
    "check_same_case(pairs)\n",
    "get_case_tfidf_scores(files,pairs)\n",
    "get_num_quotes(files,pairs)\n",
    "binarize_quotes(pairs)\n",
    "check_years(files,pairs)\n",
    "add_judge_checks(files,pairs)\n",
    "add_bins(files, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do k-fold validation on train set to identify best hyperparameters:\n",
    "\n",
    "importlib.reload(model_code)\n",
    "from model_code import get_k_fold_model_dev_pairs, save_model_df_pairs\n",
    "\n",
    "model_df_pairs = get_k_fold_model_dev_pairs(pairs)\n",
    "save_model_df_pairs(model_df_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model_code)\n",
    "from model_code import apply_models_to_dfs\n",
    "apply_models_to_dfs(model_df_pairs, infer_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model_code)\n",
    "from model_code import apply_models_to_dfs\n",
    "apply_models_to_dfs(model_df_pairs, infer_type=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "importlib.reload(model_code)\n",
    "from model_code import build_train_model\n",
    "\n",
    "train_df = pairs[pairs['set']=='train']\n",
    "model, train_df = build_train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final results\n",
    "\n",
    "importlib.reload(model_code)\n",
    "from model_code import inference_on_test\n",
    "\n",
    "test_df = pairs[pairs['set']=='test']\n",
    "\n",
    "for infer_type in [1,2]:\n",
    "    results_df = inference_on_test(model, test_df, infer_type)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
